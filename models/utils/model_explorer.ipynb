{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with the following conda environment: `../../conda_envs/training_env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model to explore\n",
    "model_folder = \"../../results/5_pk/tcp_udp/initial5min_pruning+feat_selection+quantization_0dryrun_caida/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pheavy = True if \"pheavy\" in model_folder else False\n",
    "pheavy_npk = 5\n",
    "train_minutes = 5 # Training on minutes 0-4, testing on minutes 5-59.\n",
    "update_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from statistics import fmean\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l $model_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_folder + \"args_train_val_continual_voting_pipeline.json\") as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "# Explore run args\n",
    "pprint(list(args.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "if \"_caida\" in model_folder:\n",
    "    minute = \"134500\"\n",
    "if \"_mawi\" in model_folder:\n",
    "    minute = \"1915\"\n",
    "if \"_uni\" in model_folder:\n",
    "    minute = \"145\"\n",
    "\n",
    "if is_pheavy:\n",
    "    with open(model_folder + \"model_pheavy.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "else:\n",
    "    with open(model_folder + f\"cl_pipeline_{minute}.pkl\", \"rb\") as f: # Updated model\n",
    "        model = pickle.load(f)\n",
    "\n",
    "# Sizes (for pHeavy, the sizes are saved in the model pickle file)\n",
    "if not is_pheavy:\n",
    "    with open(model_folder + f\"cl_pipeline_{minute}_sizes.pkl\", \"rb\") as f:\n",
    "        sizes_and_thr = pickle.load(f)\n",
    "\n",
    "# Metrics\n",
    "with open(model_folder + \"minute_APscore_initial_vs_CL.pkl\", \"rb\") as f:\n",
    "    scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore scores recorded during testing\n",
    "pprint(list(scores.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_pheavy:\n",
    "    if \"_none\" in model_folder:\n",
    "        with open(\"../training/params/feature_names_5pk.txt\") as file:\n",
    "            feature_names = [line.rstrip() for line in file]\n",
    "    else:\n",
    "        feature_names = None\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "    plot_tree(model[\"model\"][0], max_depth=3, ax=ax, feature_names=feature_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_pheavy:\n",
    "    print(f\"ML pipeline steps: {list(model.named_steps.keys())}\")\n",
    "    print(f\"Random forest max depth: {model['model'].max_depth}\")\n",
    "    print(f\"Random forest ntrees (after pruning): {len(model['model'].estimators_)}\")\n",
    "    pprint(sizes_and_thr)\n",
    "else:\n",
    "    for k, v in model.items():\n",
    "        print(f\"pHeavy at {k} packets: {v['size']} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "if is_pheavy:\n",
    "    ax.plot(\n",
    "        range(train_minutes, len([minute[5] for minute in scores[\"initial_model_AP\"]]) + train_minutes), \n",
    "        [minute[5] for minute in scores[\"initial_model_AP\"]], \n",
    "        label=f\"pHeavy AP (5 pk)\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(train_minutes, len([minute[5] for minute in scores[\"initial_model_F1\"]]) + train_minutes),\n",
    "        [minute[5] for minute in scores[\"initial_model_F1\"]], \n",
    "        label=f\"pHeavy F1 (5 pk)\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(train_minutes, len([minute[pheavy_npk] for minute in scores[\"initial_model_AP\"]]) + train_minutes), \n",
    "        [minute[pheavy_npk] for minute in scores[\"initial_model_AP\"]], \n",
    "        label=f\"pHeavy AP ({pheavy_npk} pk)\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(train_minutes, len([minute[pheavy_npk] for minute in scores[\"initial_model_F1\"]]) + train_minutes),\n",
    "        [minute[pheavy_npk] for minute in scores[\"initial_model_F1\"]], \n",
    "        label=f\"pHeavy F1 ({pheavy_npk} pk)\"\n",
    "    )\n",
    "else:\n",
    "    ax.plot(\n",
    "        range(train_minutes, len(scores[\"initial_model_AP\"]) + train_minutes), \n",
    "        scores[\"initial_model_AP\"], \n",
    "        label=\"initial_model_AP\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(train_minutes, len(scores[\"cl_model_AP\"]) + train_minutes), \n",
    "        scores[\"cl_model_AP\"], \n",
    "        label=\"cl_model_AP\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(train_minutes, len(scores[\"initial_model_F1\"]) + train_minutes), \n",
    "        scores[\"initial_model_F1\"], \n",
    "        label=\"initial_model_F1\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(train_minutes, len(scores[\"cl_model_F1\"]) + train_minutes), \n",
    "        scores[\"cl_model_F1\"], \n",
    "        label=\"cl_model_F1\"\n",
    "    )\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
