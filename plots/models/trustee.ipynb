{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with the following conda environment: `../../conda_envs/training_env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.tree import plot_tree \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn import tree\n",
    "\n",
    "import graphviz\n",
    "from IPython.display import display\n",
    "\n",
    "from trustee import ClassificationTrustee\n",
    "from trustee.report.trust import TrustReport\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from multiprocess import Pool\n",
    "from functools import partial\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../models/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from utils.helpers import get_X_y_binary\n",
    "from training.voting_rf_classifier import VotingRandomForestClassifier\n",
    "from utils.model_sizing import size, get_tree, get_tree_estimator, get_subforest_estimators\n",
    "from utils.model_sizing import get_best_trees_idx\n",
    "from utils.plots import plot_tree_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"../../results/\"\n",
    "data_folder = \"../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "force_redo = True\n",
    "SEED = 42\n",
    "n_pk = 5\n",
    "model_path = f\"{results_folder}/5_pk/tcp_udp/initial5min_none_0dryrun_caida/cl_pipeline_134500.pkl\"\n",
    "trustee_perf_path = model_path[:-4] + \"_trustee_perf.pkl\"\n",
    "trustee_fidelity_path = model_path[:-4] + \"_trustee_fidelity.pkl\"\n",
    "top_k = 3\n",
    "max_impurity = 0.8\n",
    "num_iter = 50\n",
    "num_stability_iter = 10\n",
    "# proba_thr = 0.04296875\n",
    "proba_thr = 0.00048828125 # From log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_train = [\n",
    "    f\"{data_folder}/caida/preprocessed_5-20pk_tcpudpicmp/130000_tcp_udp.csv\",\n",
    "    f\"{data_folder}/caida/preprocessed_5-20pk_tcpudpicmp/130100_tcp_udp.csv\",\n",
    "    f\"{data_folder}/caida/preprocessed_5-20pk_tcpudpicmp/130200_tcp_udp.csv\",\n",
    "    f\"{data_folder}/caida/preprocessed_5-20pk_tcpudpicmp/130300_tcp_udp.csv\",\n",
    "    f\"{data_folder}/caida/preprocessed_5-20pk_tcpudpicmp/130400_tcp_udp.csv\",\n",
    "]\n",
    "traces_test = [\n",
    "    f\"{data_folder}/caida/preprocessed_5-20pk_tcpudpicmp/135000_tcp_udp.csv\",\n",
    "]\n",
    "with open(\"../../models/training/params/feature_names_5pk.txt\") as f:\n",
    "    features = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, features, n_pk):\n",
    "    \"\"\"Get the input dataframe and labels\"\"\"\n",
    "\n",
    "    X_preproc, y = get_X_y_binary(\n",
    "        filenames=train, \n",
    "        feature_names=features, \n",
    "        min_packets=n_pk,\n",
    "        percentile=99,\n",
    "        verbose=True,\n",
    "    )\n",
    "    X_df = pd.DataFrame(\n",
    "        data=X_preproc,\n",
    "        columns=features\n",
    "    )\n",
    "    \n",
    "    return X_df, y\n",
    "    \n",
    "X_train, y_train = get_data(traces_train, features, n_pk)\n",
    "X_test, y_test = get_data(traces_test, features, n_pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ap = average_precision_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print(f\"{model_ap=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create our own Trustee, in order to tune pruning (max_impurity) and fitting (average_precision, proba_thresh)\n",
    "\n",
    "from trustee.main import _check_if_trained\n",
    "from trustee.utils.tree import top_k_prune\n",
    "from trustee.utils.dataset import convert_to_df, convert_to_series\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class MyClassificationTrustee(ClassificationTrustee):\n",
    "    \"\"\" A ClassificationTrustee that allows to tune \n",
    "    the max_impurity imposed when getting the stable student models\"\"\"    \n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        print(\"Init MyClassificationTrustee\")\n",
    "        if kwargs.get(\"scoring_func\"):\n",
    "            scoring_func = kwargs.pop(\"scoring_func\")\n",
    "            self.scoring_func = scoring_func\n",
    "            print(f\"Using scoring func: {self.scoring_func}\")\n",
    "        else:\n",
    "            self.scoring_func = f1_score\n",
    "        return super().__init__(**kwargs)\n",
    "    \n",
    "    @_check_if_trained\n",
    "    def get_stable(self, top_k=10, threshold=0.9, sort=True, max_impurity=0.1, proba_thr=None, predict_proba=False):\n",
    "        \"\"\"\n",
    "        Filters out explanations from Trustee stability analysis with less than threshold agreement.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        top_k: int, default=10\n",
    "            Number of top-k branches, sorted by number of samples per branch, to keep after finding\n",
    "            decision tree with highest fidelity.\n",
    "\n",
    "        threshold: float, default=0.9\n",
    "            Remove any student decision tree explanation if their mean agreement goes below given threshold.\n",
    "            To keep all students regardless of mean agreement, pass 0.\n",
    "\n",
    "        sort: bool, default=True\n",
    "            Boolean indicating whether to sort returned stable student explanation based on mean agreement.\n",
    "\n",
    "        max_impurity: float, default=0.1\n",
    "            Float indicating the maximum impurity allowed in a branch. Branch below with impurity will be pruned.\n",
    "            (So it acts more as a *min_impurity* actually)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        stable_explanations: array-like of tuple\n",
    "            [(dt, pruned_dt, agreement, reward), ...]\n",
    "\n",
    "            - dt: {DecisionTreeClassifier, DecisionTreeRegressor}\n",
    "                Unconstrained fitted student model.\n",
    "\n",
    "            - pruned_dt: {DecisionTreeClassifier, DecisionTreeRegressor}\n",
    "                Top-k pruned fitted student model.\n",
    "\n",
    "            - agreement: float\n",
    "                Mean agreement of pruned student model with respect to others.\n",
    "\n",
    "            - reward: float\n",
    "                Fidelity of student model to the expert model.\n",
    "        \"\"\"\n",
    "        if len(self._stable_students) == 0:\n",
    "            agreement = []\n",
    "            # Calculate pair-wise agreement of all top students generated during inner loop\n",
    "            # print(f\"{self._top_students=}\")\n",
    "            for i, _ in enumerate(self._top_students):\n",
    "                agreement.append([])\n",
    "                # Apply top-k pruning before calculating agreement\n",
    "                base_tree = top_k_prune(self._top_students[i][0], top_k=top_k, max_impurity=max_impurity)\n",
    "                for j, _ in enumerate(self._top_students):\n",
    "                    # Apply top-k pruning before calculating agreement\n",
    "                    iter_tree = top_k_prune(self._top_students[j][0], top_k=top_k, max_impurity=max_impurity)\n",
    "\n",
    "                    if proba_thr or predict_proba:\n",
    "                        iter_y_pred = iter_tree.predict_proba(self._X_test.values)[:, 1]\n",
    "                        base_y_pred = base_tree.predict_proba(self._X_test.values)[:, 1]\n",
    "                        if proba_thr:\n",
    "                            iter_y_pred = np.where(iter_y_pred < proba_thr, 0, 1)\n",
    "                            base_y_pred = np.where(base_y_pred < proba_thr, 0, 1)\n",
    "                    else:\n",
    "                        iter_y_pred = iter_tree.predict(self._X_test.values)\n",
    "                        base_y_pred = base_tree.predict(self._X_test.values)\n",
    "\n",
    "                    agreement[i].append(self._score(iter_y_pred, base_y_pred))\n",
    "\n",
    "                # Save complete dt, top-k prune dt, mean agreement and fidelity\n",
    "                self._stable_students.append(\n",
    "                    (\n",
    "                        self._top_students[i][0],\n",
    "                        base_tree,\n",
    "                        np.mean(agreement[i]),\n",
    "                        self._top_students[i][1],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        stable = self._stable_students\n",
    "        if threshold > 0:\n",
    "            stable = filter(lambda item: item[2] >= threshold, stable)\n",
    "\n",
    "        if sort:\n",
    "            return sorted(stable, key=lambda item: item[2], reverse=True)\n",
    "\n",
    "        return stable\n",
    "    \n",
    "    @_check_if_trained\n",
    "    def explain(self, top_k=10, max_impurity=0.1, proba_thr=None, predict_proba=False):\n",
    "        \"\"\"\n",
    "        Returns explainable model that best imitates Expert model, based on highest mean agreement and highest fidelity.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        top_student: tuple\n",
    "            (dt, pruned_dt, agreement, reward)\n",
    "\n",
    "            - dt: {DecisionTreeClassifier, DecisionTreeRegressor}\n",
    "                Unconstrained fitted student model.\n",
    "\n",
    "            - pruned_dt: {DecisionTreeClassifier, DecisionTreeRegressor}\n",
    "                Top-k pruned fitted student model.\n",
    "\n",
    "            - agreement: float\n",
    "                Mean agreement of pruned student model with respect to others.\n",
    "\n",
    "            - reward: float\n",
    "                Fidelity of student model to the expert model.\n",
    "\n",
    "            - max_impurity: float, default=0.1\n",
    "                Float indicating the maximum impurity allowed in a branch. Branch below with impurity will be pruned.\n",
    "                (So it acts more as a *min_impurity* actually)\n",
    "        \"\"\"\n",
    "        stable = self.get_stable(top_k=top_k, threshold=0, sort=False, max_impurity=max_impurity, proba_thr=proba_thr, predict_proba=predict_proba)\n",
    "        return max(stable, key=lambda item: item[2])\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        top_k=10,\n",
    "        max_leaf_nodes=None,\n",
    "        max_depth=None,\n",
    "        ccp_alpha=0.0,\n",
    "        train_size=0.7,\n",
    "        num_iter=50,\n",
    "        num_stability_iter=5,\n",
    "        num_samples=2000,\n",
    "        samples_size=None,\n",
    "        use_features=None,\n",
    "        predict_method_name=\"predict\",\n",
    "        optimization=\"fidelity\",  # for comparative purposes only\n",
    "        aggregate=True,  # for comparative purposes only\n",
    "        verbose=False,\n",
    "        max_impurity=0.1,\n",
    "        predict_proba=False,\n",
    "        proba_thr=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains Decision Tree Regressor to imitate Expert model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Internally, it will be converted to a pandas DataFrame.\n",
    "\n",
    "        y: array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values for X (class labels in classification, real numbers in regression).\n",
    "            Internally, it will be converted to a pandas Series.\n",
    "\n",
    "        top_k: int, default=10\n",
    "            Number of top-k branches, sorted by number of samples per branch, to keep after finding\n",
    "            decision tree with highest fidelity.\n",
    "\n",
    "        max_leaf_nodes: int, default=None\n",
    "            Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as\n",
    "            relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "        max_depth: int, default=None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure.\n",
    "\n",
    "        ccp_alpha: float, default=0.0\n",
    "            Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the\n",
    "            largest cost complexity that is smaller than ccp_alpha will be chosen. By default,\n",
    "            no pruning is performed. See Minimal Cost-Complexity Pruning here for details:\n",
    "            https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning\n",
    "\n",
    "        train_size: float or int, default=0.7\n",
    "            If float, should be between 0.0 and 1.0 and represent the proportion of the dataset\n",
    "            to include in the train split. If int, represents the absolute number of train samples.\n",
    "\n",
    "        num_iter: int, default=50\n",
    "            Number of iterations to repeat Trustee inner-loop for.\n",
    "\n",
    "        num_stability_iter: int, default=5\n",
    "            Number of stability to repeat Trustee stabilization outer-loop for.\n",
    "\n",
    "        num_samples: int, default=2000\n",
    "            The absolute number of samples to fetch from the training dataset split to train the\n",
    "            student decision tree model. If the `samples_size` argument is provided, this arg is\n",
    "            ignored.\n",
    "\n",
    "        samples_size: float, default=None\n",
    "            The fraction of the training dataset to use to train the student decision tree model.\n",
    "            If None, the value is automatically set to the `num_samples` provided value.\n",
    "\n",
    "        use_features: array-like, default=None\n",
    "            Array-like of integers representing the indexes of features from the `X` training samples.\n",
    "            If not None, only the features indicated by the provided indexes will be used to train the\n",
    "            student decision tree model.\n",
    "\n",
    "        predict_method_name: str, default=\"predict\"\n",
    "            The method interface to use to get predictions from the expert model.\n",
    "            If no value is passed, the default `predict` interface is used.\n",
    "\n",
    "        optimization: {\"fidelity\", \"accuracy\"}, default=\"fidelity\"\n",
    "            The comparison criteria to optimize the decision tree students in Trustee inner-loop.\n",
    "            Used for ablation study only.\n",
    "\n",
    "        aggregate: bool, default=True\n",
    "            Boolean indicating whether dataset aggregation should be used in Trustee inner-loop.\n",
    "            Used for ablation study only.\n",
    "\n",
    "        verbose: bool, default=False\n",
    "            Boolean indicating whether to log messages.\n",
    "\n",
    "        max_impurity: float, default=0.1\n",
    "            Float indicating the maximum impurity allowed in a branch. Branch below with impurity will be pruned.\n",
    "            (So it acts more as a *min_impurity* actually)\n",
    "        \n",
    "        predict_proba: bool, default=False\n",
    "            Whether to use the positive class predicted probability or the class prediction\n",
    "        \n",
    "        proba_thr: bool, default=None\n",
    "            Whether to apply a fixed cutoff of the positive class predicted probability to get the final class assignement.\n",
    "            Note: requires arg predict_proba=True\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            self.log(f\"Initializing training dataset using {self.expert} as expert model\")\n",
    "\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(\"Features (X) and target (y) values should have the same length.\")\n",
    "\n",
    "        # convert data to np array to facilitate processing\n",
    "        X = convert_to_df(X)\n",
    "        y = convert_to_series(y)\n",
    "\n",
    "        # split input array to train DTs and evaluate agreement\n",
    "        self._X_train, self._X_test, self._y_train, self._y_test = train_test_split(X, y, train_size=train_size)\n",
    "\n",
    "        features = self._X_train\n",
    "        targets = convert_to_series(getattr(self.expert, predict_method_name)(self._X_train, thresh=proba_thr))\n",
    "\n",
    "        if hasattr(targets, \"shape\") and len(targets.shape) >= 2:\n",
    "            targets = targets.ravel()\n",
    "\n",
    "        student = self.student_class(\n",
    "            random_state=0, max_leaf_nodes=max_leaf_nodes, max_depth=max_depth, ccp_alpha=ccp_alpha\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            self.log(f\"Expert model score: {self._score(self._y_train, targets)}\")\n",
    "            self.log(f\"Initializing Trustee outer-loop with {num_stability_iter} iterations\")\n",
    "\n",
    "        # Trustee outer-loop\n",
    "        for i in range(num_stability_iter):\n",
    "            self._students_by_iter.append([])\n",
    "            if verbose:\n",
    "                self.log(\"#\" * 10, f\"Outer-loop Iteration {i}/{num_stability_iter}\", \"#\" * 10)\n",
    "                self.log(f\"Initializing Trustee inner-loop with {num_stability_iter} iterations\")\n",
    "\n",
    "            # Trustee inner-loop\n",
    "            for j in range(num_iter):\n",
    "                if verbose:\n",
    "                    self.log(\"#\" * 10, f\"Inner-loop Iteration {j}/{num_iter}\", \"#\" * 10)\n",
    "\n",
    "                dataset_size = len(features)\n",
    "                size = int(int(len(self._X_train)) * samples_size) if samples_size else num_samples\n",
    "                # Step 1: Sample predictions from training dataset\n",
    "                if verbose:\n",
    "                    self.log(\n",
    "                        f\"Sampling {size} points from training dataset with ({len(features)}, {len(targets)}) entries\"\n",
    "                    )\n",
    "\n",
    "                samples_idxs = np.random.choice(dataset_size, size=size, replace=False)\n",
    "                X_iter, y_iter = features.iloc[samples_idxs], targets.iloc[samples_idxs]\n",
    "                X_iter_train, X_iter_test, y_iter_train, y_iter_test = train_test_split(\n",
    "                    X_iter, y_iter, train_size=train_size\n",
    "                )\n",
    "\n",
    "                X_train_student = X_iter_train\n",
    "                X_test_student = X_iter_test\n",
    "                if use_features is not None:\n",
    "                    X_train_student = X_iter_train.iloc[:, use_features]\n",
    "                    X_test_student = X_iter_test.iloc[:, use_features]\n",
    "\n",
    "                # Step 2: Training DecisionTreeRegressor with sampled data\n",
    "                student.fit(X_train_student.values, y_iter_train.values)\n",
    "                if predict_proba or proba_thr:\n",
    "                    student_pred = student.predict_proba(X_test_student.values)[:, 1]\n",
    "                    if proba_thr:\n",
    "                        student_pred = np.where(student_pred < proba_thr, 0, 1)\n",
    "                else:\n",
    "                    student_pred = student.predict(X_test_student.values)\n",
    "\n",
    "                if verbose:\n",
    "                    self.log(\n",
    "                        f\"Student model {i}-{j} trained with depth {student.get_depth()} \"\n",
    "                        f\"and {student.get_n_leaves()} leaves:\"\n",
    "                    )\n",
    "                    self.log(f\"Student model score: {self._score(y_iter_test, student_pred)}\")\n",
    "\n",
    "                # Step 3: Use expert model predictions to aggregate original dataset\n",
    "                if proba_thr:\n",
    "                    expert_pred = pd.Series(self.expert.predict_proba(X_iter_test)[:, 1])\n",
    "                    expert_pred = expert_pred.where(expert_pred < proba_thr, 0)\n",
    "                    expert_pred = expert_pred.where(expert_pred >= proba_thr, 1)\n",
    "                else:\n",
    "                    expert_pred = pd.Series(getattr(self.expert, predict_method_name)(X_iter_test))\n",
    "                if hasattr(expert_pred, \"shape\") and len(expert_pred.shape) >= 2:\n",
    "                    expert_pred = expert_pred.ravel()\n",
    "\n",
    "                if aggregate:\n",
    "                    features = pd.concat([features, X_iter_test])\n",
    "                    targets = pd.concat([targets, expert_pred])\n",
    "\n",
    "                if optimization == \"accuracy\":\n",
    "                    # Step 4: Calculate reward based on Decision Tree Classifier accuracy\n",
    "                    reward = self._score(y_iter_test, student_pred)\n",
    "                else:\n",
    "                    # Step 4: Calculate reward based on Decision Tree Classifier fidelity to the Expert model\n",
    "                    reward = self._score(expert_pred, student_pred)\n",
    "\n",
    "                if verbose:\n",
    "                    self.log(f\"Student model {i}-{j} fidelity: {reward}\")\n",
    "\n",
    "                # Save student to list of iterations dt\n",
    "                self._students_by_iter[i].append((deepcopy(student), reward))\n",
    "\n",
    "            # Save student with highest fidelity to list of top students by iteration\n",
    "            self._top_students.append(max(self._students_by_iter[i], key=lambda item: item[1]))\n",
    "\n",
    "        # Get best overall student based on mean agreement\n",
    "        self._best_student = self.explain(top_k=top_k, max_impurity=max_impurity, proba_thr=proba_thr, predict_proba=predict_proba)[0]\n",
    "\n",
    "\n",
    "    def _score(self, y_true, y_pred, kwargs={\"average\": \"macro\"}):\n",
    "        \"\"\"\n",
    "        Score function for student models. Compares the ground-truth predictions\n",
    "        of a blackbox model with the predictions of a student model, using self.scoring_func (cf. constructor args).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The ground-truth target values (class labels in classification, real numbers in regression).\n",
    "\n",
    "        y_pred: array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The predicted target values (class labels in classification, real numbers in regression).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score: float\n",
    "            Calculated F1-score between student model predictions and expert model ground-truth.\n",
    "        \"\"\"\n",
    "        return self.scoring_func(y_true, y_pred, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the most performant student\n",
    "\n",
    "Based on F1 score on thresholded predicted probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not force_redo:\n",
    "    try: \n",
    "        with open(trustee_perf_path, \"rb\") as f:\n",
    "            trustee_perf = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        trustee_perf = MyClassificationTrustee(expert=model, scoring_func=f1_score)\n",
    "        trustee_perf.fit(\n",
    "            X_train, y_train,\n",
    "            top_k=top_k, \n",
    "            num_iter=num_iter, \n",
    "            num_stability_iter=num_stability_iter, \n",
    "            samples_size=0.7, \n",
    "            max_impurity=max_impurity,\n",
    "            proba_thr=proba_thr,\n",
    "            verbose=False,\n",
    "            aggregate=False,\n",
    "            optimization=\"accuracy\",\n",
    "        )\n",
    "        print()\n",
    "else:\n",
    "    trustee_perf = MyClassificationTrustee(expert=model, scoring_func=f1_score)\n",
    "    trustee_perf.fit(\n",
    "        X_train, y_train,\n",
    "        top_k=top_k, \n",
    "        num_iter=num_iter, \n",
    "        num_stability_iter=num_stability_iter, \n",
    "        samples_size=0.7, \n",
    "        max_impurity=max_impurity,\n",
    "        proba_thr=proba_thr,\n",
    "        verbose=False,\n",
    "        aggregate=False,\n",
    "        optimization=\"accuracy\",\n",
    "    )\n",
    "    print()\n",
    "        \n",
    "dt, _, agreement, reward = trustee_perf.explain()\n",
    "print(f\"Model explanation training (agreement, reward): ({agreement}, {reward})\")\n",
    "print(f\"Model Explanation size: {dt.tree_.node_count}\")\n",
    "pruned_dt = trustee_perf.prune(top_k=10, max_impurity=0.01)\n",
    "print(f\"Top-k Prunned Model explanation size: {pruned_dt.tree_.node_count}\")\n",
    "\n",
    "# Evaluate perf of the explainers\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test, thresh=proba_thr)\n",
    "dt_y_pred_proba = dt.predict_proba(X_test.values)[:, 1]\n",
    "dt_y_pred = np.where(dt_y_pred_proba < proba_thr, 0, 1)\n",
    "pruned_dt_y_pred_proba = pruned_dt.predict_proba(X_test.values)[:, 1]\n",
    "pruned_dt_y_pred = np.where(pruned_dt_y_pred_proba < proba_thr, 0, 1)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 initial model: {f1}\")\n",
    "f1_student = f1_score(y_test, dt_y_pred)\n",
    "print(f\"F1 student model: {f1_student}\")\n",
    "f1_student_pruned = f1_score(y_test, pruned_dt_y_pred)\n",
    "print(f\"F1 pruned student model: {f1_student_pruned}\")\n",
    "\n",
    "ap = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"AP initial model: {ap}\")\n",
    "ap_student = average_precision_score(y_test, dt_y_pred_proba)\n",
    "print(f\"AP student model: {ap_student}\")\n",
    "ap_student_pruned = average_precision_score(y_test, pruned_dt_y_pred_proba)\n",
    "print(f\"AP pruned student model: {ap_student_pruned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trustee_perf_path, \"wb\") as f:\n",
    "    pickle.dump(trustee_perf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node importance\n",
    "top_nodes = trustee_perf.get_top_nodes(top_k=3)\n",
    "top_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "top_feat = trustee_perf.get_top_features(top_k=5)\n",
    "top_feat_named = [\n",
    "    (features[top_f[0]], top_f[1])\n",
    "    for top_f \n",
    "    in top_feat\n",
    "]\n",
    "top_feat_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trustee_perf.prune(top_k=25, max_impurity=0.01)\n",
    "print(f\"Proba thr: {proba_thr}\")\n",
    "plot_tree_thr(\n",
    "    a, \n",
    "    proba_thr, \n",
    "    6,\n",
    "    features, \n",
    "    \"./out/11app_trustee_perf.pdf\"\n",
    ")\n",
    "\n",
    "# Evaluate perf of the explainer\n",
    "pruned_dt_y_pred_proba = a.predict_proba(X_test.values)[:, 1]\n",
    "pruned_dt_y_pred = np.where(pruned_dt_y_pred_proba < proba_thr, 0, 1)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 initial model: {f1}\")\n",
    "f1_student_pruned = f1_score(y_test, pruned_dt_y_pred)\n",
    "print(f\"F1 pruned student model: {f1_student_pruned}\")\n",
    "\n",
    "ap = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"AP initial model: {ap}\")\n",
    "ap_student_pruned = average_precision_score(y_test, pruned_dt_y_pred_proba)\n",
    "print(f\"AP pruned student model: {ap_student_pruned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the most fidel student \n",
    "\n",
    "Based on F1-score on thresholded predicted probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not force_redo:\n",
    "    try: \n",
    "        with open(trustee_fidelity_path, \"rb\") as f:\n",
    "            trustee_fid = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        trustee_fid = MyClassificationTrustee(expert=model, scoring_func=f1_score)\n",
    "        trustee_fid.fit(\n",
    "            X_train, y_train,\n",
    "            top_k=top_k, \n",
    "            num_iter=num_iter, \n",
    "            num_stability_iter=num_stability_iter, \n",
    "            samples_size=0.7, \n",
    "            # samples_size=0.5, \n",
    "            max_impurity=max_impurity,\n",
    "            proba_thr=proba_thr,\n",
    "            verbose=True,\n",
    "            aggregate=False,\n",
    "            optimization=\"fidelity\",\n",
    "        )\n",
    "        print()\n",
    "else:\n",
    "    trustee_fid = MyClassificationTrustee(expert=model, scoring_func=f1_score)\n",
    "    trustee_fid.fit(\n",
    "        X_train, y_train,\n",
    "        top_k=top_k, \n",
    "        num_iter=num_iter, \n",
    "        num_stability_iter=num_stability_iter, \n",
    "        samples_size=0.5, \n",
    "        max_impurity=max_impurity,\n",
    "        proba_thr=proba_thr,\n",
    "        verbose=False,\n",
    "        aggregate=False,\n",
    "        optimization=\"fidelity\",\n",
    "    )\n",
    "    print()\n",
    "        \n",
    "dt, _, agreement, reward = trustee_fid.explain()\n",
    "print(f\"Model explanation training (agreement, reward): ({agreement}, {reward})\")\n",
    "print(f\"Model Explanation size: {dt.tree_.node_count}\")\n",
    "pruned_dt = trustee_fid.prune(top_k=10, max_impurity=0.01)\n",
    "print(f\"Top-k Prunned Model explanation size: {pruned_dt.tree_.node_count}\")\n",
    "\n",
    "# Evaluate perf of the explainers\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test, thresh=proba_thr)\n",
    "dt_y_pred_proba = dt.predict_proba(X_test.values)[:, 1]\n",
    "dt_y_pred = np.where(dt_y_pred_proba < proba_thr, 0, 1)\n",
    "pruned_dt_y_pred_proba = pruned_dt.predict_proba(X_test.values)[:, 1]\n",
    "pruned_dt_y_pred = np.where(pruned_dt_y_pred_proba < proba_thr, 0, 1)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 initial model: {f1}\")\n",
    "f1_student = f1_score(y_test, dt_y_pred)\n",
    "print(f\"F1 student model: {f1_student}\")\n",
    "f1_student_pruned = f1_score(y_test, pruned_dt_y_pred)\n",
    "print(f\"F1 pruned student model: {f1_student_pruned}\")\n",
    "\n",
    "ap = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"AP initial model: {ap}\")\n",
    "ap_student = average_precision_score(y_test, dt_y_pred_proba)\n",
    "print(f\"AP student model: {ap_student}\")\n",
    "ap_student_pruned = average_precision_score(y_test, pruned_dt_y_pred_proba)\n",
    "print(f\"AP pruned student model: {ap_student_pruned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trustee_fidelity_path, \"wb\") as f:\n",
    "    pickle.dump(trustee_fid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node importance\n",
    "top_nodes = trustee_fid.get_top_nodes(top_k=3)\n",
    "top_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "top_feat = trustee_fid.get_top_features(top_k=5)\n",
    "top_feat_named = [\n",
    "    (features[top_f[0]], top_f[1])\n",
    "    for top_f \n",
    "    in top_feat\n",
    "]\n",
    "top_feat_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trustee_fid.prune(top_k=7, max_impurity=0.01)\n",
    "proba_thr = 0.043\n",
    "print(f\"Proba thr: {proba_thr}\")\n",
    "plot_tree_thr(\n",
    "    a, \n",
    "    proba_thr, \n",
    "    15,\n",
    "    features, \n",
    "    \"./out/11app_trustee_fidelity.pdf\"\n",
    ")\n",
    "\n",
    "# Evaluate perf of the explainer\n",
    "pruned_dt_y_pred_proba = a.predict_proba(X_test.values)[:, 1]\n",
    "pruned_dt_y_pred = np.where(pruned_dt_y_pred_proba < proba_thr, 0, 1)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 initial model: {f1}\")\n",
    "f1_student_pruned = f1_score(y_test, pruned_dt_y_pred)\n",
    "print(f\"F1 pruned student model: {f1_student_pruned}\")\n",
    "\n",
    "ap = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"AP initial model: {ap}\")\n",
    "ap_student_pruned = average_precision_score(y_test, pruned_dt_y_pred_proba)\n",
    "print(f\"AP pruned student model: {ap_student_pruned}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
