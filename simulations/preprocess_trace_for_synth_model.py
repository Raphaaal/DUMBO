"""
Parse a pcap to generate ground truth files and other accessory files to be used by the synthetic model simulator.
Generated files:
- results/simu_output/{trace}/{tcp|all}/top_{}_pct/fse/memory_{}MB/gt/baseline.csv      # generated by the simulator, real size of all flows in the trace
- {pcap_path}/{pcap_name}.{tcp|all}.gt.csv                                              # like the above, but with labels (top 1% flows labeled as heavy hitters)
- {pcap_path}/{pcap_name}.{tcp|all}.gt.json                                             # alpha values to be used by the synth model to simulate a certain ap score
- {pcap_path}/{pcap_name}.{tcp|all}.{hh_perc}_thresholds.csv                            # thresholds to apply to classify {args.hh_perc} as heavy hitters
- {pcap_path}/{pcap_name}.{tcp|all}.{hh_perc}_fnr_to_fpr.csv                            # fpr to pair to simulated fnr to classify {args.hh_perc} as heavy hitters
- {pcap_path}/{pcap_name}.{tcp|all}.{hh_perc}_fpr_to_fnr.csv                            # fnr to pair to simulated fpr to classify {args.hh_perc} as heavy hitters
"""

import pandas as pd
import numpy as np
import json
import argparse
import subprocess
import math

from sklearn.metrics import average_precision_score


parser = argparse.ArgumentParser()
parser.add_argument("--results_dir", type=str, required=True)
parser.add_argument("--pcap", type=str, required=True)
parser.add_argument("--hh_perc", type=float, required=True)
parser.add_argument("--tcp_only", required=False, default=False, action='store_true', help="only consider tcp traffic")
args = parser.parse_args()


# run baseline to determine ground truth
cmd_string = f"python ./simulations/run_simulation_fse.py --type baseline --pcap {args.pcap} --hh_perc {args.hh_perc} --proba_threshold 0.5 {'--tcp_only' if args.tcp_only else ''}"
p = subprocess.Popen([ cmd_string ], shell=True)
p.wait()

trace_prefix = args.pcap[:-5]
proto = 'tcp' if args.tcp_only else 'all'
trace_set, trace_name = args.pcap.split('/')[-2:]
trace_id = f"{trace_set}-{trace_name.split('.')[0]}"
output_dir = f"{args.results_dir}/simu_output/{trace_id}/{proto}/top_{args.hh_perc}_pct/fse/memory_1.0MB"

ground_truth_file = f'{output_dir}/gt/baseline.csv'
df = pd.read_csv(ground_truth_file, names=['ip_src', 'ip_dst', 'port_src','port_dst', 'proto', 'size'])

# calculate the threshold for the top 1% of entries
threshold = df['size'].quantile(0.99)

# create a new column with 1 for entries above the threshold and 0 otherwise
df['heavy_hitter'] = (df['size'] > threshold).astype(int)

# save ground truth
df.to_csv(f'{trace_prefix}.{proto}.gt.csv', index=False)

# filter out flows with less than k packets
large_df = df[df['size'] >= 5]
large_fraction = len(large_df)/len(df)
df = large_df

# build dataset
df['key'] = df['ip_src'] + ',' + df['ip_dst'] + ',' + df['port_src'].astype(str) + ',' + df['port_dst'].astype(str) + ',' + df['proto'].astype(str)
dataset = df.set_index('key')['heavy_hitter'].to_dict()
y_true = np.array([v for v in dataset.values()])


# [AP score metric]

# synthetic model predict function
def simulate_predict(key, alpha=3):
    y_true = dataset[key]
    if y_true == 1:
        y_proba = 1 - np.random.beta(alpha, 3)
    else:
        y_proba = np.random.beta(alpha, 3)
    return y_proba

def powspace(xmin, xmax, n=50, power=2):
    xm = (xmax - xmin) / 2
    x = np.linspace(-1, 1, n)
    return np.sign(x) * abs(x) ** (power) * xm + (xmin + xmax) / 2

# determine model parameter for target AP scores
alpha_to_ap = {}
for alpha in powspace(0.01, 2.5, n=400, power=2):
    y_proba = np.array([simulate_predict(k, alpha) for k in dataset.keys()])
    alpha_to_ap[alpha] = average_precision_score(y_true, y_proba)
json.dump(alpha_to_ap, open(f'{trace_prefix}.{proto}.gt.json', 'w'))

# determine thresholds for classifying desired % of flows as elephants
APs = [x/100 for x in range(10, 100, 10)] + [0.99]
d = {AP: np.mean([alpha for alpha, ap in alpha_to_ap.items() if math.isclose(ap, AP, abs_tol=1.5e-2)]) for AP in APs}
ap_to_threshold = {}
q = 1 - (float(args.hh_perc)/large_fraction)
for ap, alpha in d.items():
    y_proba = np.array([simulate_predict(k, alpha) for k in dataset.keys()])
    ap_to_threshold[ap] = np.quantile(y_proba, q)
    #print(f"{ap} {np.quantile(y_proba, q)}")
df_thresholds = pd.DataFrame(ap_to_threshold.items(), columns=['ap','threshold'])
df_thresholds.to_csv(f'{trace_prefix}.{proto}.{args.hh_perc}_thresholds.csv', index=False)


# [Misprediction metrics]

def compute_fnr(_fpr, _tp_frac, _pp_frac):
    return ((1-_tp_frac)*_fpr - _pp_frac + _tp_frac)/_tp_frac

def compute_fpr(_fnr, _tp_frac, _pp_frac):
    return (_tp_frac*_fnr + _pp_frac - _tp_frac)/(1 - _tp_frac)

def simulate_predict_from_rates(key, fnr, fpr):
    y_true = dataset[key]
    if y_true == 1:
        y_pred = int(np.random.rand() > fnr)
    else:
        y_pred = int(np.random.rand() < fpr)
    return y_pred


pp_frac = float(args.hh_perc)/large_fraction    # fraction of flows in the k=5 dataset that corresponds to hh_perc of the total
tp_frac = y_true[y_true == 1].size/y_true.size  # fraction of flows in the k=5 dataset that is elephant

# vary FNR, determine FPR for pp_frac
fnr_to_fpr = []
for rate in [x/100 for x in range(0, 105, 5)]:
    fnr = rate
    fpr = compute_fpr(fnr, tp_frac, pp_frac)
    y_proba = np.array([simulate_predict_from_rates(k, fnr, fpr) for k in dataset.keys()])
    ap_score = average_precision_score(y_true, y_proba)
    print(f"FNR: {fnr:.4f} FPR: {fpr:f} AP: {ap_score:.2f}")
    fnr_to_fpr.append((fnr, fpr, ap_score))

df_misprediction_rates = pd.DataFrame(fnr_to_fpr, columns=['fnr','fpr', 'ap'])
df_misprediction_rates.to_csv(f'{trace_prefix}.{proto}.{args.hh_perc}_fnr_to_fpr.csv', index=False)

print(f"-")
fpr_to_fnr = {}
# vary FPR, determine FNR for pp_frac
# minimum FPR is pp_frac-tp_frac (need some false positive to reach the desired fraction)
# maximum FPR is pp_frac (cannot predict more than the desired fraction)
min_fpr = float(f'{fnr_to_fpr[0][1]*2:.2f}')/2
min_fpr = min_fpr + 0.005 if min_fpr < fnr_to_fpr[0][1] else min_fpr
for delta in [0.0, 0.005, 0.01, 0.015, 0.02, 0.025]: # for mawi: [0.0, 0.03, 0.06, 0.09, 0.012, 0.15]:
    fpr = min_fpr + delta
    fnr = compute_fnr(fpr, tp_frac, max(pp_frac, fpr))
    print(f"FPR: {fpr:.4f} FNR: {fnr:f}")
    fpr_to_fnr[fpr] = fnr

df_misprediction_rates = pd.DataFrame(fpr_to_fnr.items(), columns=['fpr','fnr'])
df_misprediction_rates.to_csv(f'{trace_prefix}.{proto}.{args.hh_perc}_fpr_to_fnr.csv', index=False)
